10 May 2024 (v2.1.0)
- `Agent.assign_functions()` now takes in Python functions without need to parse into `Function` class first - output format, description and input variables will be inferred from function signature and docstring
- `Function()` now allows just parsing the entire thing from external function, using `Function(external_fn = fn)`
- Created Tutorial 6: External Function Interface to demonstrate the new function interfacing capabilities for TaskGen Agents and interfacing with LangChain and LlamaIndex
- Better prompting for `get_next_subtask` to be more detailed in the next subtask
- Made `use_llm()` aware that it is called `use_llm`, so it doesn't return an error saying it does not have that function

9 May 2024 (v2.0.1)
- Fixed a bug with `async_strict_json` to incorporate one `async_chat()` call instead of `chat()`

9 May 2024 (v2.0.0)
- HUGE: Revamped `get__next_subtask` to remove overall plan array, and make LLM only do single step lookahead (works much better without much hallucination of tasks)
- HUGE: Async support added. Added async_chat, async_strict_json, async Function call in base.py
- Reprompted `reply_user()` to respond to task completion status better if there is no query
- Reformatted `Global Context:` in prompt to make it enclose everything in `get_global_context()`
- Made context of `use_llm()` function better for better generation

5 May 2024 (v1.4.0)
- Changed prompt of strict_json to make it compatible with more LLMs and improve performance on ChatGPT: 
    '''\nOutput in the following json string format: {new_output_format}
    Update text enclosed in <>. Output only a valid json string beginning with {{ and ending with }}'''
- Fixed an error with input parameter mismatch in remove() in Memory
- Added more external LLM examples in Tutorial 0
- Updated all Tutorials

22 Apr 2024 (v1.3.2)
- Changes requirements.txt to dill>=0.3.7 so that it can run with colab
- Added return_as_json = True as an input variable to strict_json (default: False, so it will be a python dictionary), so that you can get the json string as output if needed. Works also for OpenAIJson mode

16 Apr 2024 (v1.3.1)
- Changed requirements.txt to openai>=1.3.6 so that it is compatible with newer versions of LangChain

26 Mar 2024 (v1.3.0)
- Changed `get_additional_context` to `get_global_context` to better reflect that it is information that carries over across tasks
- Refined Tutorial 5 Chatbot with Sherlock Holmes example to make it more performant with some prompt changes

22 Mar 2024 (v1.2.0)
- Changed StrictJSON prompt to make it better: Added "You must output valid json with all keys present." to system prompt of `strict_json` to make it more robust
- Added `is_compulsory` variable in `Function`. Default: `False`. When set to `True`, make them always appear for planning regardless of whether RAG is used
- Added more examples for memory

18 Mar 2024 (v1.1.0)
- Made the llm variable more robust by explicitly referencing it in the calls from `Agent` to `Function` to `strict_json`
- Added agent loading and saving using `save_agent` and `load_agent`
- Added `get_additional_context` for persistent variables and additional prompts to be given to `Agent`
- Ensure that in `Ranker`, the query and key are always string to prevent unhashable types from being referenced by dictionary
- If function takes in no input, in `next_subtasks_completed()` we will not force the LLM to output the parameter and skip the step for input parameter type conformity

3 Mar 2024 (v1.0.0)
- Better prompt engineering for `use_llm`, `reply_user`, `get_next_subtask`
- Subtasks completed index now a string so as to store unhashable datatypes too
- Added functionality for removing `use_llm` and `end_task` as functions of Agent (in case you want to do manual execution of functions)
- Now allowing for duplicate subtask names by adding (count) at the end of the subtask instruction -> needed for OS-like usage\
- Equipped Function in `get_next_subtask()` now uses an Enum to force the answer into a legitimate assigned function
- `get_next_subtask()` now generates the Equipped Function Inputs separately for non-instruction-based inputs, allowing for all input fields and input types to be enforced by strictjson
- Better distinguished between Subtask and Overall Task in `use_llm`, so that `use_llm` should only focus on the current Subtask and not solve Overall Task directly
- Fixed issue with nested outputs of nested dictionaries / lists after I added the true/false to True/False auto conversion

1 Mar 2024 (v0.0.8)
- Added automatic parsing of fn_description from external function's docstring if fn_description is not provided for `Function()`
- type checks now support `array`: same functionality as `list`, but LLM may understand `array` better due to JSON formatting type
- Auto type conversion from list to array after `type:` in `output_format`

29 Feb 2024 (v0.0.7)
- Throws an exception when `type:` is given in `output_format` for `Function()` when using External Functions as we do not do type checking there
- Throws a similar exception when `type:` is given in `output_format` for `openai_json_mode = True` in `strict_json` or `Function`

28 Feb 2024 (v0.0.6)
- Made list processing in StrictJSON more robust

26 Feb 2024 (v0.0.5)
- Added memory and RAG on functions and additional context based on task / overall plan

25 Feb 2024 (v0.0.4)
- Added shared variables, enabling a pool of shared variables to persist between functions for better handling of non-text modalities as well as long text
- Refined the prompt for get_next_subtask() in order to do element by element check of whether the Overall Plan is completed
- Changed the dictionary parsing of StrictJSON to ensure that we extract the leftmost and rightmost { and } respectively to do ast.literal_eval. Previously might have the quotation marks at the left and right which affected parsing
- Changed the bool parsing of StrictJSON to convert true/false to True/False first so as to make it parseable by ast.literal_eval

22 Feb 2024 (v0.0.3)
- Added hierarchical agents
- Refined prompts for use_llm and reply_user
- Agents now use subtasks_completed as a memory for context for future generation

17 Feb 2024 (v0.0.1)
- Creation of TaskGen, a task-based agentic framework building on StrictJSON outputs by LLM agents